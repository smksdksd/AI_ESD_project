import streamlit as st
import os
import tempfile
import hashlib
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --------------------------------------------------------------------------
# 1. í˜ì´ì§€ ì„¤ì • ë° ë‹¤êµ­ì–´ UI í…ìŠ¤íŠ¸ ì •ì˜
# --------------------------------------------------------------------------

st.set_page_config(page_title="AI ë…¼ë¬¸ ë¶„ì„ Q&A", layout="wide")

LANGUAGES = {
    "í•œêµ­ì–´": "ko",
    "English": "en",
    "æ—¥æœ¬èª": "ja",
    "ä¸­æ–‡": "zh"
}
ANSWER_LANGUAGES = {
    "Auto/ë¬¸ì„œ ê¸°ì¤€": "auto",
    "í•œêµ­ì–´": "Korean",
    "English": "English",
    "æ—¥æœ¬èª": "Japanese",
    "ä¸­æ–‡": "Chinese"
}

def get_ui_labels(lang_code):
    if lang_code == "en":
        return {
            "title": "ğŸ“„ AI Paper Analysis & Q&A",
            "upload_header": "1. Settings & Upload",
            "file_uploader": "Select a file (PDF, DOCX, TXT)",
            "analyze_btn": "Start Analysis",
            "analyzing": "Analyzing the paper... This may take a moment.",
            "analyze_success": "Analysis complete! You can now ask questions.",
            "analyze_error": "Error during analysis:",
            "upload_first": "Please upload a file first.",
            "ask_header": "2. Ask Questions",
            "ask_placeholder": "e.g., What are the main contributions of this paper?",
            "wait_answer": "Generating answer with ChatGPT...",
            "answer_header": "ğŸ¤– ChatGPT Answer",
            "answer_error": "Error during answer generation:",
            "need_upload": "Please upload and analyze a paper first.",
            "history_header": "Q&A History",
            "already_analyzed": "This paper has been analyzed.",
            "current_paper": "Currently analyzing:",
            "ui_lang_label": "ğŸŒ UI Language",
            "ans_lang_label": "ğŸ¤– Answer Language"
        }
    elif lang_code == "ja":
        return {
            "title": "ğŸ“„ AIè«–æ–‡åˆ†æ & Q&A",
            "upload_header": "1. è¨­å®šã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            "file_uploader": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ (PDF, DOCX, TXT)",
            "analyze_btn": "è§£æé–‹å§‹",
            "analyzing": "è«–æ–‡ã‚’è§£æä¸­ã§ã™...ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚",
            "analyze_success": "è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼è³ªå•ã§ãã¾ã™ã€‚",
            "analyze_error": "è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š",
            "upload_first": "ã¾ãšãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
            "ask_header": "2. è³ªå•ã™ã‚‹",
            "ask_placeholder": "ä¾‹ï¼šã“ã®è«–æ–‡ã®ä¸»ãªè²¢çŒ®ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "wait_answer": "ChatGPTãŒå›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...",
            "answer_header": "ğŸ¤– ChatGPTã®å›ç­”",
            "answer_error": "å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š",
            "need_upload": "ã¾ãšè«–æ–‡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£æã—ã¦ãã ã•ã„ã€‚",
            "history_header": "Q&Aå±¥æ­´",
            "already_analyzed": "ã“ã®è«–æ–‡ã¯ã™ã§ã«è§£æã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "current_paper": "ç¾åœ¨è§£æä¸­ã®è«–æ–‡ï¼š",
            "ui_lang_label": "ğŸŒ UIè¨€èª",
            "ans_lang_label": "ğŸ¤– å›ç­”è¨€èª"
        }
    elif lang_code == "zh":
        return {
            "title": "ğŸ“„ AIè®ºæ–‡åˆ†æä¸é—®ç­”",
            "upload_header": "1. è®¾ç½®ä¸ä¸Šä¼ ",
            "file_uploader": "è¯·é€‰æ‹©æ–‡ä»¶ (PDF, DOCX, TXT)",
            "analyze_btn": "å¼€å§‹åˆ†æ",
            "analyzing": "æ­£åœ¨åˆ†æè®ºæ–‡â€¦â€¦è¯·ç¨å€™ã€‚",
            "analyze_success": "åˆ†æå®Œæˆï¼ç°åœ¨å¯ä»¥æé—®ã€‚",
            "analyze_error": "åˆ†ææ—¶å‡ºé”™ï¼š",
            "upload_first": "è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚",
            "ask_header": "2. æé—®",
            "ask_placeholder": "ä¾‹å¦‚ï¼šè¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ",
            "wait_answer": "ChatGPTæ­£åœ¨ç”Ÿæˆç­”æ¡ˆâ€¦â€¦",
            "answer_header": "ğŸ¤– ChatGPTç­”æ¡ˆ",
            "answer_error": "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼š",
            "need_upload": "è¯·å…ˆä¸Šä¼ å¹¶åˆ†æè®ºæ–‡ã€‚",
            "history_header": "Q&Aè®°å½•",
            "already_analyzed": "è¯¥è®ºæ–‡å·²è¢«åˆ†æã€‚",
            "current_paper": "å½“å‰åˆ†æçš„è®ºæ–‡ï¼š",
            "ui_lang_label": "ğŸŒ UIè¯­è¨€",
            "ans_lang_label": "ğŸ¤– ç­”æ¡ˆè¯­è¨€"
        }
    else:  # "ko"
        return {
            "title": "ğŸ“„ AI ë…¼ë¬¸ ë¶„ì„ ë° Q&A",
            "upload_header": "1. ì„¤ì • ë° ì—…ë¡œë“œ",
            "file_uploader": "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (PDF, DOCX, TXT)",
            "analyze_btn": "ë…¼ë¬¸ ë¶„ì„ ì‹œì‘",
            "analyzing": "ë…¼ë¬¸ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...",
            "analyze_success": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "analyze_error": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:",
            "upload_first": "ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
            "ask_header": "2. ë…¼ë¬¸ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê¸°",
            "ask_placeholder": "ì˜ˆ: ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "wait_answer": "ChatGPTê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...",
            "answer_header": "ğŸ¤– ChatGPT ë‹µë³€",
            "answer_error": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ:",
            "need_upload": "ë¨¼ì € ë…¼ë¬¸ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.",
            "history_header": "Q&A ê¸°ë¡",
            "already_analyzed": "ì´ë¯¸ ë¶„ì„ëœ ë…¼ë¬¸ì…ë‹ˆë‹¤.",
            "current_paper": "í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ë…¼ë¬¸:",
            "ui_lang_label": "ğŸŒ UI ì–¸ì–´",
            "ans_lang_label": "ğŸ¤– ë‹µë³€ ì–¸ì–´"
        }

# --------------------------------------------------------------------------
# ì´í•˜ ì½”ë“œëŠ” ê¸°ì¡´ê³¼ ë™ì¼ (ìƒëµ)
# --------------------------------------------------------------------------
# --------------------------------------------------------------------------
# 2. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜ (ìºì‹± ì ìš©)
# --------------------------------------------------------------------------

def get_file_hash(file_obj):
    """íŒŒì¼ì˜ ë‚´ìš©ìœ¼ë¡œ MD5 í•´ì‹œë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    file_bytes = file_obj.getvalue()
    return hashlib.md5(file_bytes).hexdigest()

@st.cache_data(show_spinner=False)
def get_text_from_doc(file_bytes, filename):
    """ì—…ë¡œë“œëœ íŒŒì¼ ë°”ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    suffix = os.path.splitext(filename)[-1].lower()
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
        tmp_file.write(file_bytes)
        tmp_file_path = tmp_file.name
    try:
        if suffix == ".pdf":
            loader = PyPDFLoader(tmp_file_path)
        elif suffix == ".docx":
            loader = Docx2txtLoader(tmp_file_path)
        elif suffix == ".txt":
            loader = TextLoader(tmp_file_path, encoding='utf-8')
        else:
            raise ValueError(f"Unsupported file type: {suffix}")
        pages = loader.load_and_split()
        text = " ".join(t.page_content for t in pages)
    finally:
        os.remove(tmp_file_path)
    return text

@st.cache_resource(show_spinner=False)
def create_qa_chain(_text, answer_language):
    """í…ìŠ¤íŠ¸ì™€ ë‹µë³€ ì–¸ì–´ë¥¼ ë°›ì•„ LangChain QA ì²´ì¸ì„ ìƒì„±í•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    chunks = text_splitter.split_text(_text)
    embeddings = OpenAIEmbeddings(openai_api_key=st.secrets["OPENAI_API_KEY"])
    vector_store = FAISS.from_texts(chunks, embeddings)
    prompt_template = (
        "Use the following pieces of context to answer the question at the end.\n"
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n"
        "{context}\n\nQuestion: {question}\n"
    )
    if answer_language != "auto":
        prompt_template += f"\nHelpful Answer (MUST be in {answer_language}):"
    else:
        prompt_template += "\nHelpful Answer:"
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7, openai_api_key=st.secrets["OPENAI_API_KEY"])
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(),
        chain_type_kwargs={"prompt": PROMPT}
    )

# --------------------------------------------------------------------------
# 3. Streamlit ì„¸ì…˜ ìƒíƒœ(Session State) ì´ˆê¸°í™”
# --------------------------------------------------------------------------

if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "history" not in st.session_state:
    st.session_state.history = []
if "analyzed_filehash" not in st.session_state:
    st.session_state.analyzed_filehash = None
if "analyzed_filename" not in st.session_state:
    st.session_state.analyzed_filename = None
if "language" not in st.session_state:
    st.session_state.language = "ko"
if "answer_language" not in st.session_state:
    st.session_state.answer_language = "auto"
if "last_answer_language" not in st.session_state:
    st.session_state.last_answer_language = st.session_state.answer_language

# --------------------------------------------------------------------------
# 4. ë©”ì¸ í™”ë©´ UI êµ¬ì„±
# --------------------------------------------------------------------------

labels = get_ui_labels(st.session_state.language)
st.title(labels["title"])
st.markdown("---")

st.header(labels["upload_header"])
col1, col2 = st.columns(2)
with col1:
    selected_lang_key = st.selectbox(
        labels["ui_lang_label"],
        options=list(LANGUAGES.keys()),
        index=list(LANGUAGES.values()).index(st.session_state.language)
    )
    st.session_state.language = LANGUAGES[selected_lang_key]
with col2:
    selected_ans_lang_key = st.selectbox(
        labels["ans_lang_label"],
        options=list(ANSWER_LANGUAGES.keys()),
        index=list(ANSWER_LANGUAGES.values()).index(st.session_state.answer_language)
    )
    st.session_state.answer_language = ANSWER_LANGUAGES[selected_ans_lang_key]

# UI ì–¸ì–´ ë³€ê²½ ì‹œ ë¼ë²¨ ì¦‰ì‹œ ì—…ë°ì´íŠ¸
labels = get_ui_labels(st.session_state.language)

uploaded_file = st.file_uploader(
    labels["file_uploader"],
    type=['pdf', 'docx', 'txt'],
    help="ìµœëŒ€ 20MB íŒŒì¼ ê¶Œì¥ (ìš©ëŸ‰ì´ í´ìˆ˜ë¡ ë¶„ì„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
)
analyze_button = st.button(labels["analyze_btn"], type="primary", use_container_width=True)

answer_language_changed = st.session_state.last_answer_language != st.session_state.answer_language

if analyze_button:
    if uploaded_file is not None:
        current_file_hash = get_file_hash(uploaded_file)
        if current_file_hash != st.session_state.analyzed_filehash or answer_language_changed:
            with st.spinner(labels["analyzing"]):
                try:
                    file_bytes = uploaded_file.getvalue()
                    extracted_text = get_text_from_doc(file_bytes, uploaded_file.name)
                    st.session_state.qa_chain = create_qa_chain(extracted_text, st.session_state.answer_language)
                    st.session_state.analyzed_filename = uploaded_file.name
                    st.session_state.analyzed_filehash = current_file_hash
                    st.session_state.history = []
                    st.session_state.last_answer_language = st.session_state.answer_language
                    st.success(labels["analyze_success"])
                    st.rerun()
                except Exception as e:
                    st.error(f"{labels['analyze_error']} {str(e)[:300]}")
        else:
            st.info(labels["already_analyzed"])
    else:
        st.warning(labels["upload_first"])

st.markdown("---")

# --------------------------------------------------------------------------
# 5. Q&A ë° ê¸°ë¡ í‘œì‹œ ì„¹ì…˜
# --------------------------------------------------------------------------

if st.session_state.qa_chain:
    st.header(labels["ask_header"])
    # íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ì¤‘ê°„ ìƒëµ
    max_filename_len = 36
    show_filename = (
        st.session_state.analyzed_filename if st.session_state.analyzed_filename and len(st.session_state.analyzed_filename) <= max_filename_len
        else (st.session_state.analyzed_filename[:16] + "..." + st.session_state.analyzed_filename[-16:]) if st.session_state.analyzed_filename else "ì—†ìŒ"
    )
    st.markdown(f"**{labels['current_paper']}** `{show_filename}`")

    with st.form(key="question_form", clear_on_submit=True):
        query = st.text_input(
            "ì§ˆë¬¸:",
            placeholder=labels["ask_placeholder"],
            label_visibility="collapsed"
            # autofocus=True  # TypeError ë°©ì§€ë¥¼ ìœ„í•´ ì˜µì…˜ ì œê±°
        )
        submitted = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°")

    if submitted and query:
        with st.spinner(labels["wait_answer"]):
            try:
                response = st.session_state.qa_chain.invoke(query)
                st.session_state.history.append({"question": query, "answer": response['result']})
            except Exception as e:
                st.error(f"{labels['answer_error']} {str(e)[:300]}")
                st.session_state.history.append({"question": query, "answer": f"Error: {e}"})

    if st.session_state.history:
        st.subheader(labels["history_header"])
        MAX_Q_LEN = 60
        total = len(st.session_state.history)
        for i, qa in enumerate(reversed(st.session_state.history)):
            short_q = qa['question'] if len(qa['question']) <= MAX_Q_LEN else qa['question'][:MAX_Q_LEN] + "..."
            q_number = total - i
            with st.expander(f"Q{q_number}: {short_q}"):
                st.markdown(f"**A:** {qa['answer']}")
        # Q&A ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        st.download_button(
            "Q&A ê¸°ë¡ ë‹¤ìš´ë¡œë“œ",
            data="\n\n".join([f"Q: {h['question']}\nA: {h['answer']}" for h in st.session_state.history]),
            file_name="qa_history.txt"
        )
else:
    st.info(labels["need_upload"])
